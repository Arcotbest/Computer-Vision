# -*- coding: utf-8 -*-
"""EmergencyVehical classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1zMKEgEM6_-pBdvu3PttZ_cQQJJqvXI
"""



"""We need upload the data into google drive.
from there we need procss.

steps google drive.

We need to Build  drive with 

    from google.colab import drive

    drive.mount('/content/drive')

we need to set path for the project

    cd emergency_classification

    cd images
"""

from google.colab import drive
drive.mount('/content/drive')

"""This below command will set a path to your Drive

    cd drive/My\ Drive
"""

cd drive/My\ Drive

# to get present working directoy by pwd
pwd

ls

cd emergency_classification

cd images

"""We are trying to read sample and display"""

path='/content/drive/My Drive/emergency_classification/images/'
imageName = "1.jpg";
myimage=path+imageName

"""Display sample image , that ensure you have done set up google drive to colab"""

import matplotlib.pyplot as plt
img = plt.imread(myimage)
img

plt.imshow(img)
print(img.shape)

"""intially we have single image. by using glob method we can bring all images in to google colab memeory

from glob import glob
      images = glob(path+"*.jpg")
"""

from glob import glob

path='/content/drive/My Drive/emergency_classification/images/'

images = glob(path+"*.jpg")

images[1:10]

"""we are now going. to see the random images. to generate radomness we use

import numpy as np
        rng = np.random.RandomState()
"""

import numpy as np
rng = np.random.RandomState()

rng.choice(images)

"""Below code will give us different image every time when we execute because of

      img_name = rng.choice(images)
"""



img_name = rng.choice(images)
img = plt.imread(img_name)
plt.imshow(img)
print(img_name)

import pandas as pd
data = pd.read_csv("/content/drive/My Drive/emergency_classification/emergency_classification.csv")

data.tail()

"""We need to get the class counts such that we ensure that data for project is balanced .

Thumb rule is for Deep Learning we need to have balance data.

        Mean Class A have n images , Class should have n images

        if this voilated deep learning tries to learns the features of majority class which will results wrong predictions.

        even though you imbalnce data we can 99.99% accuracy but it will learn majority of class.

Blacend means Class A = 128
                      Class B = 99

        imbalanced  Class A = 128
                    Class B = 28

        Basic Thumb rule is your data should be equal portion of different classes
"""

data['emergency_or_not'].value_counts()
y = data['emergency_or_not'].values

data['emergency_or_not'].value_counts()

"""In this project we dont equal portion of data but still we have it can said to balanced ."""



path='/content/drive/My Drive/emergency_classification/images/'
row_index = rng.choice(data.index)
img_name = data.iloc[row_index]['image_names']
img = plt.imread(path+img_name)
plt.imshow(img)
target = data.iloc[row_index]['emergency_or_not']

if (target == 1 ):
    print("Emergency")
else :
        print("Non Emergency")

"""Most important thing we need convert all images in array"""

path="/content/drive/My Drive/emergency_classification/images/"
imageName = "1.jpg";


X = []
for img_name in data['image_names']:
    img = plt.imread(path + img_name)
    X.append(img)

X = np.array(X)
X.shape

"""We converted images into Array. so size become total records along with size"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
from glob import glob 

from tensorflow.keras.layers import Dense

# Set the seed and create random number generator object
seed = 42
rng = np.random.RandomState(seed)
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import GlobalMaxPooling2D

from sklearn.model_selection import train_test_split
X_train ,X_test, y_train , y_test = train_test_split( X, y,test_size=0.3, random_state=seed)

X_train.shape

X_test.shape

# For Model building in Tensorflow we us Method sequential
model = Sequential()
# We need to specify the Length of input image , wher use paramete of Input_shape
model.add(InputLayer(input_shape=(224, 224, 3)))#X[0].shape))
# for this base model we adding convolutional with filter of 25 and kernal size = (5,5)
model.add(Conv2D(filters=25,kernel_size=(3,3),activation='relu',strides=(1,1),padding='valid'))
model.add(Conv2D(filters=50,kernel_size=(3,3),activation='relu',strides=(1,1),padding='valid'))
# Concverting into 2D to 1D.
model.add(Flatten())
model.add(Dense(units=100,activation='sigmoid'))
#Final layer we can use classifiere in this case we have two classes we are using activition function as sigmiod.
model.add(Dense(units=1,activation='sigmoid'))

# Next import thing is nueral network building is Compile . we are using image data so have to specify loss fucntion 
# approprite to problem

model.compile(loss='binary_crossentropy',optimizer='sgd', metrics=['accuracy'])

#Optimizer along with learning rate

model.summary()

"""Model total paramaters will help us understanding the model depth"""

model.fit(X_train,y_train,epochs=10,batch_size=120,validation_data=(X_test,y_test),verbose=2)

"""Test Accuracy : 56%
        Training Accuracy : 58%

        This is clear indication underfitting
"""



"""Train the mode by changing the hyper parameters
Model 2Â¶
"""

model2=Sequential()
model2.add(InputLayer(input_shape=(224, 224, 3)))#X[0].shape))
model2.add(Conv2D(filters=25,kernel_size=(3,3),activation='relu',strides=(1,1),padding='valid'))
model2.add(Conv2D(filters=50,kernel_size=(3,3),activation='relu',strides=(1,1),padding='valid'))
# We are trying to improve model accuaracy . so we using our model parameter tunning
model2.add(MaxPooling2D(pool_size=(2,2),padding='valid'))
model2.add(Flatten())
model2.add(Dense(units=100,activation='sigmoid'))
model2.add(Dense(units=1,activation='sigmoid'))
model2.compile(loss='binary_crossentropy',optimizer='sgd', metrics=['accuracy'])

model2.summary()

model2.fit(X_train,y_train,epochs=20,batch_size=256,validation_data=(X_train,y_train),verbose=2)

"""For Models there is no improvements for models
              We are going to use batch normalization
              Dropout

## TUNED MODEL
"""

from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dropout

model3 = Sequential()
model3.add(InputLayer(input_shape=(224, 224, 3)))

model3.add(Conv2D(filters=125,kernel_size=(5,5),activation='relu',strides=(2,2),padding='same'))
model3.add(Conv2D(filters=150,kernel_size=(5,5),activation='relu',strides=(2,2),padding='same'))
model3.add(BatchNormalization())
model3.add(MaxPooling2D(pool_size=(8,8),padding='valid'))
#model3.add(Dropout(0.3))

model3.add(Conv2D(filters=200,kernel_size=(3,3),activation='relu',strides=(2,2),padding='same'))
model3.add(Conv2D(filters=225,kernel_size=(3,3),activation='relu',strides=(2,2),padding='same'))
model3.add(BatchNormalization())
model3.add(Conv2D(filters=250,kernel_size=(3,3),activation='relu',strides=(2,2),padding='same'))
model3.add(Conv2D(filters=275,kernel_size=(3,3),activation='relu',strides=(2,2),padding='same'))

model3.add(GlobalMaxPooling2D())
#model3.add(Dropout(0.3))

model3.add(Dense(units=100,activation='sigmoid'))
model3.add(Dense(units=1,activation='sigmoid'))

model3.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

model3.summary()

model3.fit(X_train,y_train,epochs=100,batch_size=128,validation_data=(X_train,y_train),verbose=2)

"""Training Accuracy : 99

Test Accuracy.    : 99
"""



"""Validation on predictions"""

path="/content/drive/My Drive/emergency_classification/images/"
# get predictions
predictions = model3.predict_classes(X_train)[:, 0]
prediction_probabilities = model3.predict(X_train)[:, 0]

# pull out the original images from the data
# which correspond to the validation data
_, test_vehicles, _, test_y = train_test_split(data.image_names.values, y, test_size=0.3, random_state=seed)

# get a random index to plot image randomly
index = rng.choice(range(len(test_vehicles)))

# get the corresponding image name and probability
img_name = test_vehicles[index]
prob = (prediction_probabilities * 100).astype(int)[index]

# read the image
img = plt.imread(path + img_name)

# print probability and actual class
print(prob , '% sure that it is emergency')
print('Whereas actual class is ', test_y[index])

# plot image
plt.imshow(img);